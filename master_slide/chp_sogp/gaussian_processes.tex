\section{Gaussian Process Regression: Bayesian Non-Parametric Model}
\subsection{Mathematical Framework}
\begin{frame}
	In the GP framework, the dataset $\mathcal{D}$ is used to learn a random mapping function $\bm{f}(\cdot)$, capturing the relationship between $\bm{x}_n$ and $\bm{y}_n$. We define a GP distribution over $\bm{f}(\cdot)$ as:
	\begin{equation}\label{mogp_notation}
	\bm{f}(\bm{x}) \sim  \mathcal{GP}\left(\bm{0},\, \bm{k}(\bm{x}, \bm{x}' \mid \bm{\theta})\right)
	\end{equation}
	where $\bm{f}: \mathcal{X} \rightarrow \mathbb{R}^D$ maps the input space, and $\bm{k}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}^{D \times D}$ is the cross-covariance matrix function, parameterized by $\bm{\theta}$. Adding i.i.d Gaussian noise $\bm{\epsilon}$ with covariance $\Sigma_\epsilon = \text{diag}\{\sigma^2_{Nd}\}_{d=1}^D$, the model prediction becomes $\bm{y}_n = \bm{f}(\bm{x}_n) + \bm{\epsilon}$.
\end{frame}

\begin{frame}{Joint Distribution}
	Let $\mathbf{X}_* = \{\bm{x}_{n*}\}_{n=1}^{N_*}$ be a set of test points $\bm{x}_{n*} \in \mathcal{X}$ with test output vector $\mathbf{f}_* = [\bm{f}(\bm{x}_1)^\top, \bm{f}(\bm{x}_2)^\top, \cdots, \bm{f}(\bm{x}_{N_*})^\top]^\top \in \mathbb{R}^{N_* D}$. The joint Gaussian distribution over $\mathbf{y}$ and $\mathbf{f}_*$ is given by:
	
	\begin{equation}\label{sogp_prior_matrix}
	\begin{array}{rcl}
	\left[ \begin{array}{c}
	\mathbf{y}\\
	\mathbf{f_*}\\
	\end{array}
	\right]
	\sim
	\mathcal{N} \left(
	\bm{0},
	\left[ \begin{array}{cc}
	\mathbf{K}_y & \mathbf{K}_*\\
	\mathbf{K}_*^\top & \mathbf{K}_{**}\\
	\end{array}
	\right] \right)
	\end{array}
	\end{equation}
	
	Here, $\mathbf{K}_y=\mathbf{K} + \Sigma_\epsilon$ with $\mathbf{K} \in \mathbb{R}^{ND \times ND}$, formed by evaluating the covariance function at all pairs in $\mathbf{X}$. The matrices $\mathbf{K}_{**} \in \mathbb{R}^{N_*D \times N_*D}$ and $\mathbf{K}_{*} \in \mathbb{R}^{ND \times N_*D}$ are formed by evaluating the covariance function across test inputs in $\mathbf{X}_*$ and test-train inputs, respectively.
\end{frame}

\begin{frame}{Posterior Distribution}
	This notation allows for deriving the conditional distribution named posterior as follows:
	
	\begin{equation}\label{sogp_posterior}
	\mathbf{f}_*|\mathbf{X}_*, \mathcal{D} \sim \mathcal{N}(\bar{\mathbf{f}_*},\text{cov}(\mathbf{f}_*))
	\end{equation}
	with
	\begin{equation}\label{sogp_posterior_mean}
	\bar{\mathbf{f}_*} = \mathbf{K}_*^\top \mathbf{K}_y^{-1} \mathbf{y}
	\end{equation}
	\begin{equation}\label{sogp_posterior_cov}
	\text{cov}(\mathbf{f}_*) = \mathbf{K}_{**} -  \mathbf{K}_*^\top \mathbf{K}_y^{-1} \mathbf{K}_*
	\end{equation}
\end{frame}

\begin{frame}{The Marginal Log-likelihood}
	 Furthermore, the prediction performance archived by the conditional distribution depends on the selected parameter set $\bm{\theta}$ and observation noise matrix $\Sigma_\epsilon$. Both parameters are calculated by maximizing the marginal log-likelihood from \Cref{sogp_prior_matrix} where $p(\mathbf{y}) = \mathcal{N}(\mathbf{y} \mid \bm{0}, \mathbf{K}_y)$ as follows:
	\begin{equation}\label{eq:sogp_nlml_opt}
	\begin{aligned}
	\{\bm{\theta}_{\text{opt}}, {\Sigma_\epsilon}_{\text{opt}}\} &= \underset{\bm{\theta}, \Sigma_\epsilon}{\arg\max} \quad \ln p(\mathbf{y})\\
	&= \underset{\bm{\theta}, \Sigma_\epsilon}{\arg\min} \quad \frac{1}{2} \mathbf{y}^\top \mathbf{K}_y^{-1} \mathbf{y} + \frac{1}{2} \ln \mid \mathbf{K}_y \mid + \frac{ND}{2} \ln 2\pi
	\end{aligned}
	\end{equation}
	
	The main challenge is their computational complexity of $\mathcal{O}(N^3D^3)$ and storage demand of $\mathcal{O}(N^2D^2)$ due to inverting the matrix $\mathbf{K}_y$.
\end{frame}

\begin{frame}{Variational Inference}
	A common approach to reduce GP complexity, consists of the introduction of a reduced set of $M \ll N$ inducing point locations, denoted by $Z$, containing vectors $\mathbf{z} \in \mathcal{X}$, and an inducing variable vector $\mathbf{u} = [\bm{f}(\bm{z}_1)^\top, \bm{f}(\bm{z}_2)^\top, \cdots, \bm{f}(\bm{z}_{M})^\top]^\top \in \mathbb{R}^{M D}$. This approach leads to the following extended joint distribution:
	\begin{equation}
	\begin{array}{rcl}
	\left[ \begin{array}{c}
	\mathbf{u}\\
	\mathbf{f}\\
	\end{array}
	\right]
	\sim
	\mathcal{N} \left(
	\begin{array}{c}
	\mathbf{0}\\
	\end{array},
	\left[ \begin{array}{cc}
	\mathbf{K}_{uu} & \mathbf{K}_{uf}\\
	\mathbf{K}_{uf}^\top & \mathbf{K}\\
	\end{array}
	\right] \right)
	\end{array}
	\end{equation}
	
	Here, $\mathbf{K}_{uu} \in \mathbb{R}^{MD \times MD}$ and $\mathbf{K}_{uf} \in \mathbb{R}^{MD \times ND}$ represent the block covariance matrices evaluated at the inducing point pairs and between the inducing points and training inputs, respectively. We call this model the Sparse Variational Gaussian Process (SVGP). 
	
\end{frame}

\begin{frame}{The Variational Distribution}
	From the above, the conditional distribution is given by:
	\begin{equation}
	p(\mathbf{f} \mid \mathbf{u}) = \mathcal{N}\left( \mathbf{f} \mid \mathbf{K}_{uf}^\top \mathbf{K}_{uu}^{-1} \mathbf{u}, \mathbf{K} - \mathbf{K}_{uf}^\top \mathbf{K}_{uu}^{-1} \mathbf{K}_{uf}\right),
	\end{equation}
	and the prior for the inducing variables is:
	\begin{equation}
	p(\mathbf{u}) = \mathcal{N}\left(\mathbf{u} \mid \mathbf{0}, \mathbf{K}_{uu} \right).
	\end{equation}
	The joint posterior distribution over the latent variable $\mathbf{f}$ and inducing variable $\mathbf{u}$ is:
	\begin{equation}
	p(\mathbf{f}, \mathbf{u} \mid \mathbf{y}) = p(\mathbf{f} \mid \mathbf{u}) p(\mathbf{u} \mid \mathbf{y}).
	\end{equation}
	However, computing $p(\mathbf{u} \mid \mathbf{y})$ is typically intractable, so we use variational inference to approximate the posterior with a variational distribution $q(\mathbf{u}) = \mathcal{N}\left( \mathbf{u} \mid \boldsymbol{\mu}, \boldsymbol{S} \right)$, where $\boldsymbol{\mu} \in \mathbb{R}^{MD}$ and $\boldsymbol{S} \in \mathbb{R}^{MD \times MD}$. The approximate joint posterior then becomes:
	\begin{equation}\label{eq:so_join_posterior}
	p(\mathbf{f}, \mathbf{u} \mid \mathbf{y}) \approx q(\mathbf{f}, \mathbf{u}) = p(\mathbf{f} \mid \mathbf{u}) q(\mathbf{u}).
	\end{equation}
	
\end{frame}

\begin{frame}{ELBO}
	Accordingly, the approximation of the posterior distribution consists of estimating the variational parameters $\boldsymbol{\mu}$ and $\boldsymbol{S}$, performed by maximizing an evidence lower bound (ELBO). Such ELBO is obtained from the log marginal likelihood:
	\begin{equation}
	\ln p(\mathbf{y}) \geq \int \int q(\mathbf{f}, \mathbf{u}) \ln \frac{p(\mathbf{y} \mid \mathbf{f}) p(\mathbf{f}\mid \mathbf{u}) p(\mathbf{u})}{q(\mathbf{f}, \mathbf{u})} d\mathbf{f} d\mathbf{u} = \mathcal{L}
	\end{equation}
	resulting from the Jensen's inequality. We can rewrite the previous expression to derive the following:
	\begin{equation}
	\mathcal{L} = \mathbb{E}_{q(\mathbf{f})}\{\ln p(\mathbf{y} \mid \mathbf{f})\} - \text{KL}\{q(\mathbf{u})\parallel p(\mathbf{u})\}
	\end{equation}
	Here, $\text{KL}$ represents the Kullback--Leibler divergence between the Gaussian-shaped distributions.
\end{frame}

\begin{frame}{ELBO}
	With the i.i.d assumption, the likelihood function $p(\mathbf{y} \mid \mathbf{f})$ can be factorized over observations and outputs:
	\begin{equation}
	p(\mathbf{y} \mid \mathbf{f}) = \prod_{d=1}^D\prod_{n=1}^N p(y_{dn} \mid f_{d}(\mathbf{x}_n)),
	\end{equation}
	where $f_{d}(\mathbf{x}_n)$ represents the latent function value for the $d$-th output at the $n$-th input, and $y_{dn}$ is the corresponding label. The lower bound can then be expressed as:
	\begin{equation}\label{eq:sogp_elbo}
	\mathcal{L} = \sum_{d=1}^D\sum_{n=1}^N \mathbb{E}_{q(f_{d}(\mathbf{x}_n))}\{\ln p(y_{dn} \mid f_{d}(\mathbf{x}_n))\} - \sum_{d=1}^D \text{KL}\{q(\mathbf{u}_d)\parallel p(\mathbf{u}_d)\}.
	\end{equation}
	
	The summation over $D$ and $N$ facilitates training through a mini-batch fashion.
\end{frame}

\begin{frame}{Posterior and Predictive Distribution}
	We define the variational distribution for $\mathbf{f}$ as:
	\begin{align}
	q(\mathbf{f}) &:= \int p(\mathbf{f} \mid \mathbf{u}) q(\mathbf{u}) d\mathbf{u} \nonumber \\
	&= \mathcal{N}\left(\mathbf{f} \mid \mathbf{K}_{uf}^\top \mathbf{K}_{uu}^{-1} \boldsymbol{\mu}, \mathbf{K} + \mathbf{K}_{uf}^\top \mathbf{K}_{uu}^{-1}(\mathbf{S} - \mathbf{K}_{uu})\mathbf{K}_{uu}^{-1}\mathbf{K}_{uf} \right).
	\end{align}
	
	This approach reduces complexity to $\mathcal{O}(NM^2D^3)$, as $\mathbf{K}_{uu}$ is smaller than $\mathbf{K}_{y}$. For predictions at new points $\mathbf{x}_*$, we compute:
	\begin{equation}
	p(\mathbf{f}_* \mid \mathbf{y}) \approx q(\mathbf{f}_*) = \int p(\mathbf{f}_* \mid \mathbf{u}) q(\mathbf{u}) d \mathbf{u},
	\end{equation}
	and adding Gaussian noise $\Sigma_\epsilon$ to the latent posterior $\mathbf{f}_* \mid \mathbf{y}$.
\end{frame}

\begin{frame}{Model Setup}
	The proposed approach factorizes the GP scalar covariance function into two kernels: $k_{\mathcal{X}}$, which models input correlations via relative distances, and $k_{D}$, which models task pair-wise correlations as follows:
	
	\begin{align}
	k\left((\bm{x}, d), (\bm{x}', d') \mid \bm{\theta}\right) &= k_{\mathcal{X}}\left(\bm{x}, \bm{x}' \mid \bm{\Theta}_d \right) k_{D}\left(d, d' \mid \sigma_d \right), \\
	k_{\mathcal{X}}\left(\bm{x}, \bm{x}' \mid \bm{\Theta}_d \right) &= \text{exp}\left(-\frac{1}{2}(\bm{x} - \bm{x}')^\top \bm{\Theta}_d^{-2} (\bm{x} - \bm{x}')\right), \\
	k_{D}\left(d, d' \mid \sigma^2_d \right) &= \sigma^2_d ~ \delta_{d, d'},
	\end{align}
	
	where $\delta_{d, d'}$ is the Kronecker delta, $\sigma^2_d$ is the output scale for task $d$, and $\bm{\Theta}_d$ is a diagonal matrix of lengthscale factors. The input kernel $k_{\mathcal{X}}$ uses a squared-exponential function, ensuring smooth data mapping. The task kernel $k_{D}$ avoids modeling task correlations directly, reducing complexity to $\mathcal{O}(N M^2 D)$. Despite not modeling task correlations explicitly, the autoregressive inputs and shared inducing points still enable exploration of task dependencies.
	
\end{frame}

\begin{frame}{Performance Metrics}
	To evaluate our model's performance, we use the following three metrics:
	\begin{itemize}
		\item Mean Squared Error (MSE)
		\item Mean Standardized Log Loss (MSLL)
		\item Continuous Ranked Probability Score (CRPS)
	\end{itemize}
\end{frame}

\begin{frame}{Mean Squared Error}
	The MSE, measures the average squared difference between the observed outcome at the $n$-th test time step for the $d$-th output, denoted as $y_{dn}$, and the expected value of the predicted outcome, denoted as $\mathbb{E}\{y_{dn*}\}$, over $N_*$ test~observations.
	\begin{equation}\label{eq:mse}
	\text{MSE} = \frac{1}{DN_{*}} \sum_{d=1}^{D} \sum_{n=1}^{N_*} (y_{dn} - \mathbb{E}\{y_{dn*}\})^2
	\end{equation}
	Note that the MSE metric does not account for the prediction uncertainty.
\end{frame}

\begin{frame}{Mean Standardized Log Loss (MSLL)}
	MSLL measures the quality of probabilistic predictions by evaluating the log probability of observed values under the predicted distribution. Assuming normally distributed errors, MSLL is given by:
	\begin{equation}
	\text{MSLL} = \frac{1}{2DN_{*}} \sum_{d=1}^{D} \sum_{n=1}^{N_*} \left( \frac{(y_{dn} - \mathbb{E}\{y_{dn*}\})^2}{\text{var}\{y_{dn*}\}} - \frac{(y_{dn} - \mu_d)^2}{\sigma_d^2} + \ln \left(\frac{\text{var}\{y_{dn*}\}}{\sigma_d^2}\right) \right)
	\end{equation}
	where $\text{var}\{y_{dn*}\}$, $\mu_d$, and $\sigma_d^2$ are the predicted variance, mean, and variance of the training data for the $d$-th output. A positive MSLL indicates worse performance than a Gaussian baseline, zero indicates parity, and negative indicates improvement.
\end{frame}


\begin{frame}{Continuous Ranked Probability Score}
	The CRPS evaluates the quality of probabilistic predictions by measuring the area of the squared difference between the predicted cumulative distribution function (CDF) $\Phi(y_{dn})$ and the empirical degenerate CDF of the observed values. For a Gaussian predictive distribution, the CRPS becomes the following:
	\begin{equation}
	\text{CRPS} = \frac{1}{DN_*} \sum_{d=1}^{D} \sum_{n=1}^{N_*} \sqrt{\text{var}\{y_{dn*}\}} \left(\beta_{dn} \left( 2\Phi\left(\beta_{dn}\right) - 1 \right) + 2\phi\left(\beta_{dn}\right) - \frac{1}{\sqrt{\pi}} \right)
	\end{equation}
	with $\beta_{dn} = \frac{y_{dn} - \mathbb{E}\{y_{dn*}\}}{\sqrt{\text{var}\{y_{dn*}\}}}$ being the zero-mean unit variance standardization of the random variable $y_{dn}$, and a standard Gaussian for predictive distribution $\phi(\beta_{dn})=\mathcal{N}(\beta_{dn}\mid 0,1)$.
\end{frame}

\subsection{Results and Discussions}

\begin{frame}{Hyperparameter Tuning}
	The task involves predicting the $D=23$ reservoir contributions at day $n+H$ using the $L=23$ contributions at day $n$. The forecasting horizon $H$ includes short-term (up to a week ahead, $H\in\{1,2,3,4,5,6,7\}$) and medium-term (up to one month ahead, $H\in\{9,14,21,30\}$) predictions. The SVGP model requires tuning the number of inducing points $M$ and order size $T$. 
\end{frame}

\begin{frame}
	 Results evidence that the median MSLL and MSE generally decrease when increasing the number of inducing points, up to $M=64$, after which the error increases, and increases for large values of $T$. This suggests underfitting with too few inducing points and overfitting with too many.
 	\begin{figure}[htbp]
	 	% \centering
	 	\setlength\figurewidth{0.45\textwidth} 
	 	\setlength\figureheight{0.45\textwidth}
 		\subfloat[MSLL ($\times10^{-2}$)]{\input{chp_sogp/figures/gs_msll}}\hspace{-1em}
 		\subfloat[MSE ($\times10^{-1}$)]{\input{chp_sogp/figures/gs_mse}}
	\end{figure}
	The optimal settings are $M=64$ and $T=1$.
\end{frame}

\begin{frame}{Reservoir-Wise Output Scales and Noise Variance}
	Using the tuned number of inducing points $M$ and model order $T$, We present the trained reservoir-wise output scales $\sigma_d^2$ and the estimated noise variance $\Sigma_\epsilon$ for different horizons ($H$).
	
	\begin{figure}[htbp]
		% \centering
	 	\setlength\figurewidth{0.45\textwidth} 
		\setlength\figureheight{0.45\textwidth}
		\subfloat[Output scales]{\input{chp_sogp/figures/output_scale_matrix}}\hspace{-1em}
		\subfloat[Noise variances]{\input{chp_sogp/figures/noise_variance_matrix}}
	\end{figure}
\end{frame}

\begin{frame}{Output Scales}
	 Overall, the longer the horizon, the smaller the output scale for all reservoirs. The above suggests a reduction in correlation for long horizons, making the SVGP approximate the prior distribution with a time-uncorrelated function. Nonetheless, for reservoirs $P$ and $V$, an output scale increment at long horizons follows the initial reduction, implying the existence of relevant periodicity components within such time series. Lastly, reservoir $Q$ consistently renders low-scale values along horizons to enforce the expected zero streamflow. 
\end{frame}

\begin{frame}{Noise Variance}

	Regarding the noise variance, the heightened unpredictability in the output data at long horizons yields an increment in $\Sigma_\epsilon$, which also supports the positive definiteness of the covariance matrix $\mathbf{K}_y$. For such horizons, the model uncertainty mainly depends on the noise variance rather than the output scale.
	
	Therefore, the SVGP model exploits the data correlations at short horizons, yielding a low uncertainty while understanding the lack of information for long-horizon forecasting as a zero-mean isotropic Gaussian posterior. 
\end{frame}

\begin{frame}{Lengthscale Analysis}
	The lengthscale analysis relies on its capacity to deactivate nonessential features on a kernel machine according to the Automatic Relevance Determination (ARD). Hence, a small lengthscale value implies a high relevance for predicting the future streamflow of a reservoir from the past of another.
	\begin{figure}[htbp]
		\centering
		\setlength\figurewidth{0.36\columnwidth} 
		\setlength\figureheight{0.36\columnwidth}
		\subfloat[$H=1.$]{\input{chp_sogp/figures/lengthscalematrix1}}\hspace{-1.3em}
		\subfloat[$H=14.$]{\input{chp_sogp/figures/lengthscalematrix14}}\hspace{-1.3em}
		\subfloat[$H=30.$]{\input{chp_sogp/figures/lengthscalematrix30}}
		\caption{Trained lengthscales from input feature (columns) to each output task (rows) for three prediction horizons.}
	\end{figure}
\end{frame}

\begin{frame}
	The shortest horizon distributes the smallest lengthscale values over the main diagonal contrary to the high off-diagonal values. This result proves the relevance of short-term memory components within each streamflow.
	
	At a longer horizon of fourteen days, the lengthscales over the main diagonal become higher, losing relevance, and several off-diagonal become lower, gaining relevance, than at a one-day horizon. Such a change implies that a reservoir provides less information to predict itself at longer horizons, leading the model to look for knowledge in other reservoirs.
	
	The longest horizon illustrates even less relevant lengthscales, indicating that the SVGP restrains the number of relevant features. Thus, the lengthscale variations across different prediction horizons reveal the dynamic interplay between reservoirs and highlight the Gaussian Process's ability to select relevant features adaptively.
	
\end{frame}

\begin{frame}{t-distributed Stochastic Neighbor Embedding (t-SNE)}
	For visual interpretation of SVGP inner workings, the following figure maps in 2D the reservoir data along with the optimized inducing points using the t-distributed Stochastic Neighbor Embedding (t-SNE) technique for each target reservoir. The filled contours outline the density distribution of training data, whereas the color scatter locates the optimized inducing points. We strategically labeled three inducing points for analyzing the SVGP behavior.
	%%%JGG: No todos los puntos de la figura están numerados.
	Since the forecasting tasks share the inducing points, not the kernel parameters, each target reservoir holds a different t-SNE mapping.
	
\end{frame}

\begin{frame}
	\begin{figure}[htbp]
		\centering
		\begin{subfigure}[t]{0.38\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{chp_sogp/figures/TSNE_task4_kde_p30.png}
			\caption{Reservoir E.}
			\label{fig:points_locations_E}
		\end{subfigure}
		\hspace{0.05\columnwidth} % Reduced space between columns
		\begin{subfigure}[t]{0.38\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{chp_sogp/figures/TSNE_task8_kde_p30.png}
			\caption{Reservoir I.}
			\label{fig:points_locations_I}
		\end{subfigure}
		
		\vspace{0.3cm} % Space between rows
		
		\begin{subfigure}[t]{0.38\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{chp_sogp/figures/TSNE_task11_kde_p30.png}
			\caption{Reservoir L.}
			\label{fig:points_locations_L}
		\end{subfigure}
		\hspace{0.05\columnwidth} % Reduced space between columns
		\begin{subfigure}[t]{0.38\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{chp_sogp/figures/TSNE_task20_kde_p30.png}
			\caption{Reservoir U.}
			\label{fig:points_locations_U}
		\end{subfigure}
		
		\vspace{0.1cm} % Space between the figures and the caption
		
		\caption{t-SNE-based two-dimensional mapping of the SVGP latent space and inducing points' locations for four target reservoirs. Three inducing points are numbered to analyze the model behavior.}
		\label{fig:inducing_points_locations}
	\end{figure}
\end{frame}

\begin{frame}
	The 2D plots exhibit multi-modal distributions for all tasks, indicating clustered information and varying dynamics within a reservoir. Hence, the SVGP trades off the location of the inducing points between the within-task centroids and the among-task relationships. Therefore, the shared inducing points allow for the capturing of task-wise, group-wise, and global information about the streamflow dynamics.

\end{frame}

\subsubsection{Performance Analysis}

\begin{frame}{Model Comparison}
	For performance analysis, the proposed SVGP-based forecasting is contrasted against the straightforward Linear AutoRegressive model (LAR) and the nonlinear Long-Short-Term Memory network (LSTM).
	
	Particularly for LAR, we trained a single output model for each task. Further, the frequentist perspective treats the model weights as random estimators following a Gaussian distribution. Such a treatment turns the LAR into a probabilistic model, allowing it to generate confidence intervals for its predictions.
	
	In the case of LSTM, we set up a single multi-output model to forecast all reservoirs simultaneously. To tune the number of hidden units of the LSTM, we run a grid search, yielding an optimum of one unit. As a deterministic model, LSTM lacks a predictive distribution and cannot be assessed with the probabilistic performance metrics MSLL and CRPS.
\end{frame}


\begin{frame}
	\begin{figure}[htbp]
		\centering
		\setlength\figurewidth{\columnwidth} 
		\setlength\figureheight{0.26\columnwidth}
		
		\input{chp_sogp/figures/SALVAJINA_forecasting}\hfill
		\input{chp_sogp/figures/AMANI_forecasting}\hfill
		\input{chp_sogp/figures/SAN LORENZO_forecasting}
		
		\caption{Test data for reservoirs T (low complexity), A (medium complexity), and I (high complexity), top to bottom in a one-day ahead model's prediction. Yellow and blue shaded areas represent the $95\%$ centered confidence interval for the LAR and SVGP predictions, respectively.}
	\end{figure}
\end{frame}


\begin{frame}{MSE Scores}
	\begin{figure}[htbp]
		%\centering
		\setlength\figurewidth{0.37\columnwidth} 
		\setlength\figureheight{0.37\columnwidth}
		\subfloat[LAR.\label{fig:mse_lar}]{\input{chp_sogp/figures/mse_lar}}\hspace{-1.5em}
		\subfloat[LSTM.\label{fig:mse_lstm}]{\input{chp_sogp/figures/mse_lstm}}\hspace{-1.5em}
		\subfloat[SVGP.\label{fig:mse_svgp}]{\input{chp_sogp/figures/mse_igp}}
		\caption{Achieved MSE for LAR, LSTM, and SVGP forecasting models at each horizon and reservoir.}
		\label{fig:mse_scores} 
	\end{figure}
\end{frame}

\begin{frame}{MSLL Score}
	\begin{figure}[htbp]
		%\centering
		\setlength\figurewidth{0.5\columnwidth} 
		\setlength\figureheight{0.445\columnwidth}
		\subfloat[LAR.\label{fig:msll_lar}]{\input{chp_sogp/figures/msll_lar}}\hspace{-1.5em}
		\subfloat[SVGP.\label{fig:msll_svgp}]{\input{chp_sogp/figures/msll_igp}}
		\caption{{Achieved MSLL} %MDPI: Please change the hyphen (-) into a minus sign ($-$, "U+2212") in the image.
			for LAR and SVGP forecasting models at each horizon and reservoir.}
		\label{fig:msll_scores} 
	\end{figure}
\end{frame}

\begin{frame}{Performance t-test}
	\tiny
	\begin{table}[htbp]
		% \centering
		\caption{Performance metrics for LAR, LSTM, and SVGP on the considered horizons $H$. Bold and asterisk denote a \emph{p}-value $p<1\%$ in a one-tailed paired \emph{t}-test for LAR vs. SVGP and LSTM vs. SVGP.}
		\label{tab:metrics}
		% \newcolumntype{C}{>{\centering\arraybackslash}X}
		\begin{tabular}{c p{1cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}}
			\toprule
			& \multicolumn{3}{c}{\textbf{MSE}} & \multicolumn{2}{c}{\textbf{MSLL}} & \multicolumn{2}{c}{\textbf{CRPS}} \\
			\cmidrule{2-8}
			\( \textbf{H} \) & \textbf{LAR} & \textbf{LSTM} & \textbf{SVGP} & \textbf{LAR} & \textbf{SVGP} & \textbf{LAR} & \textbf{SVGP} \\
			\midrule
			1  & 0.51 & 0.96 & 0.52 * & $-$0.39 & $-$0.53 & 0.34 & 0.32 \\
			2  & 0.63 & 1.01 & \textbf{0.61} * & $-$0.27 & \textbf{$-$0.42} & 0.39 & \textbf{0.36} \\
			3  & 0.68 & 1.02 & \textbf{0.65} * & $-$0.22 & \textbf{$-$0.38} & 0.41 & \textbf{0.38} \\
			4  & 0.72 & 1.03 & \textbf{0.69} * & $-$0.18 & \textbf{$-$0.34} & 0.42 & \textbf{0.39} \\
			5  & 0.74 & 1.06 & \textbf{0.71} * & $-$0.17 & \textbf{$-$0.33} & 0.43 & \textbf{0.40} \\
			6  & 0.76 & 1.07 & \textbf{0.72} * & $-$0.16 & \textbf{$-$0.32} & 0.44 & \textbf{0.40} \\
			7  & 0.76 & 1.11 & \textbf{0.74} * & $-$0.15 & \textbf{$-$0.31} & 0.44 & \textbf{0.41} \\
			14 & 0.83 & 1.12 & \textbf{0.79} * & $-$0.10 & \textbf{$-$0.27} & 0.46 & \textbf{0.43} \\
			21 & 0.88 & 1.08 & \textbf{0.83} * & $-$0.07 & \textbf{$-$0.23} & 0.48 & \textbf{0.45} \\
			30 & 0.91 & 1.06 & \textbf{0.88} * & $-$0.05 & \textbf{$-$0.20} & 0.49 & \textbf{0.46} \\
			\midrule
			Grand Average & 0.74 & 1.05 & \textbf{0.71} * & $-$0.18 & \textbf{$-$0.33} & 0.43 & \textbf{0.40} \\
			\bottomrule
		\end{tabular}
		
	\end{table}
\end{frame}

\subsubsection{Concluding Remark}
\begin{frame}{To Conclude}
	\begin{itemize}
		\justifying
		\item The proposed methodology diminishes the computational complexity from cubic to linear regarding the number of samples, becoming more scalable to forecasting tasks on large datasets.
		\item The optimal number of inducing points performs as an inherent regularization by encoding the most information from the data while avoiding overfitting.
		\item The t-SNE-based distribution plots reveal that the proposed model strategically places the shared inducing points to capture task-wise, group-specific, and global dynamics, trading off between capturing the reservoir-wise unique characteristics and the between-reservoir dependencies, improving the overall performance in streamflow forecasting.
	\end{itemize}
	
\end{frame}

\begin{frame}{To Conclude}
	\begin{itemize}
		\justifying
		\item The trained lengthscales prove that the GP model successfully adjusts its focus to prediction horizon changes between short-term memory components and long-term relationships. This adaptability makes the Gaussian Process model robust and versatile for multiple-output forecasting tasks with varying time features.
		
		\item The performance analysis evidences the advantage of the proposed SVGP model over the baseline LAR and LSTM models for streamflow prediction in three main aspects. Firstly, adaptability to changing dynamics within and between reservoirs. Secondly, the Bayesian scheme returns a posterior distribution and provides informative confidence intervals. Thirdly, the SVGP better copes with the enhanced forecasting complexity for long horizons than baseline approaches, as proved by the slower error growth.
		
	\end{itemize}
	
\end{frame}
