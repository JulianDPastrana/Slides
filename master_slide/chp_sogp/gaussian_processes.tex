\section*{Objective 1: Sparse Variational Gaussian Process}

\subsection{Mathematical Framework}

\begin{frame}{Objective 1: Sparse Variational Gaussian Process}
	In a GP framework, the function $\bm{f}(\cdot)$ maps inputs $\bm{x}_n$ to outputs $\bm{y}_n$. Adding i.i.d. Gaussian noise $\bm{\epsilon}$, the model becomes:
	
	\[
	\bm{y}_n = \bm{f}(\bm{x}_n) + \bm{\epsilon}
	\]
	
	For test inputs $\mathbf{X}_*$, the joint distribution of training outputs $\mathbf{y}$ and test outputs $\mathbf{f}_*$ is:
	
	\[
	\left[ \begin{array}{c}
		\mathbf{y}\\
		\mathbf{f_*}\\
	\end{array}
	\right] \sim \mathcal{N} \left( \bm{0}, \left[
	\begin{array}{cc}
		\mathbf{K}_y & \mathbf{K}_* \\
		\mathbf{K}_*^\top & \mathbf{K}_{**}
	\end{array}\right] \right)
	\]
	
	The posterior distribution for test points is:
	
	\[
	\mathbf{f}_* | \mathbf{X}_*, \mathcal{D} \sim \mathcal{N}(\mathbf{K}_*^\top \mathbf{K}_y^{-1} \mathbf{y}, \mathbf{K}_{**} - \mathbf{K}_*^\top \mathbf{K}_y^{-1} \mathbf{K}_*)
	\]
	
	\begin{itemize}
		\item $\mathbf{K}_y = \mathbf{K} + \Sigma_\epsilon$, where $\mathbf{K} \in \mathbb{R}^{ND \times ND}$ is the covariance matrix for the train set and $\Sigma_\epsilon$ contains task-wise noise.
		
		\item $\mathbf{K}_{**} \in \mathbb{R}^{N_* D \times N_* D}$ is the covariance matrix for the test set.
		
		\item $\mathbf{K}_* \in \mathbb{R}^{ND \times N_* D}$ represents the cross-covariance matrix between the training and test points.
	\end{itemize}
	
\end{frame}


\begin{frame}{Variational Inference for Scalability}

	The prediction performance is influenced by the selected parameter set \(\bm{\theta}\) and the matrix \(\Sigma_\epsilon\). These parameters are determined by maximizing the marginal log-likelihood:
	
	\begin{equation*}
		\{\bm{\theta}_{\text{opt}}, {\Sigma_\epsilon}_{\text{opt}}\} = \underset{\bm{\theta}, \Sigma_\epsilon}{\arg\max} \quad -\frac{1}{2} \mathbf{y}^\top \mathbf{K}_y^{-1} \mathbf{y} - \frac{1}{2} \ln \lvert \mathbf{K}_y \rvert - \frac{ND}{2} \ln 2\pi,
	\end{equation*}
	
	with complexity \textcolor{myNewColorB}{\(\mathcal{O}(N^3D^3)\)} due to the need to invert the matrix \(\mathbf{K}_y\).	For scalability, we introduce \(M \ll N\) inducing points \(Z\) with inducing variables $q(\mathbf{u}) = \mathcal{N}(\boldsymbol{\mu}, \mathbf{S})$, providing the following ELBO:
	\begin{equation*}
		\mathcal{L} = \sum_{d=1}^D \sum_{n=1}^N \mathbb{E}_{q(f_d(\mathbf{x}_n))}\{\ln p(y_{dn} \mid f_d(\mathbf{x}_n))\} - \sum_{d=1}^D \text{KL}\{q(\mathbf{u}_d) \parallel p(\mathbf{u}_d)\},
	\end{equation*}
	where $f_d(\mathbf{x}_n)$ represents the $d$-th latent function value at input $\mathbf{x}_n$, and $y_{dn}$ is the corresponding observed value. This reduces the complexity to \textcolor{myNewColorB}{$\mathcal{O}(NM^2D^3)$}. All parameters were tuned by Adam optimizer.
\end{frame}


\begin{frame}{Model Setup}
	The GP covariance is factorized into two kernels: $k_{\mathcal{X}}$ for input correlations and $k_{D}$ for task correlations:
	\[
	k\left((\bm{x}, d), (\bm{x}', d')\right) = k_{\mathcal{X}}\left(\bm{x}, \bm{x}' \mid \bm{\Theta}_d \right) k_{D}\left(d, d' \mid \sigma_d \right),
	\]
	with:
	\[
	k_{\mathcal{X}}\left(\bm{x}, \bm{x}'\right) = \exp\left(-\frac{1}{2}(\bm{x} - \bm{x}')^\top \bm{\Theta}_d^{-2} (\bm{x} - \bm{x}')\right),
	\]
	\[
	k_{D}(d, d') = \sigma^2_d \delta_{d, d'},
	\]
	where $\delta_{d, d'}$ is the Kronecker delta, $\bm{\Theta}_d=\text{diag}\{\Delta_{dl}\}_{l=1}^{DT}$ is the lengthscale matrix, and $\sigma^2_d$ is the output scale. This reduces complexity to \textcolor{myNewColorB}{$\mathcal{O}(NM^2D)$} by avoiding explicit task correlations.
\end{frame}



\subsection{Results and Discussions}

\begin{frame}{Grid search average values for tuning $T$ and $M$}
	\vspace{-1em}
	\begin{figure}[htbp]
		\setlength\figurewidth{0.55\textwidth} 
		\setlength\figureheight{0.5\textwidth}
		\subfloat[MSE ($\times10^{-1}$)]{\input{chp_sogp/figures/gs_mse}}\hspace{-1em}
		\subfloat[MSLL ($\times10^{-2}$)]{\input{chp_sogp/figures/gs_msll}}
	\end{figure}
	\vspace{-1.5em}
	\begin{itemize}
		\item The error increases with larger values of \( T \).
		\item Smaller values of \( M \) are more sensitive to initialization.
		\item Larger values of \( M \) tend to overfit.
		\item The optimal parameters are \( M = 64 \) and \( T = 1 \).
	\end{itemize}

\end{frame}

\begin{frame}{Reservoir-Wise Kernel and Likelihood Parameters}
	\vspace{-0.5em}
	\begin{figure}[htbp]
		\captionsetup{skip=-10em} % You can adjust this value as needed
		
			\centering
		\tiny
		% First row with two centered subfigures
		\setlength\figurewidth{0.4\textwidth} 
		\setlength\figureheight{0.28\textwidth}
		\subfloat[Output scales $\sigma^2_d$]{\input{chp_sogp/figures/output_scale_matrix}}
		\hspace{2em} % Adjust the space between the two figures
		\subfloat[Noise variances $\Sigma_\epsilon$]{\input{chp_sogp/figures/noise_variance_matrix}}\\ % Add space between rows
		\vspace{-1.2em}
		% Second row with three centered subfigures
		\setlength\figurewidth{0.4\textwidth} % Smaller width for three figures
		\setlength\figureheight{0.28\textwidth}
		\subfloat[$H=1$]{\input{chp_sogp/figures/lengthscalematrix1}}\hspace{-1.2em}
		\subfloat[$H=14$]{\input{chp_sogp/figures/lengthscalematrix14}}\hspace{-1.2em}
		\subfloat[$H=30$]{\input{chp_sogp/figures/lengthscalematrix30}}
	\end{figure}
	\vspace{-1em}
	\begin{itemize}
		\item For longer horizons, output scales decrease, while noise variances increase, reflecting weaker correlations and greater task complexity.
		\item As horizons extend, the relevance of main diagonal lengthscales diminishes, with off-diagonal lengthscales becoming more significant.
	\end{itemize}
	
\end{frame}


\begin{frame}{SVGP t-SNE Latent Mapping Space}
	\begin{figure}[htbp]
		\centering
		\tiny
		\begin{subfigure}[t]{0.4\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{chp_sogp/figures/TSNE_task4_kde_p30.png}
			\caption{Reservoir E}
		\end{subfigure}
		\hspace{0.05\columnwidth} % Reduced space between columns
		\begin{subfigure}[t]{0.4\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{chp_sogp/figures/TSNE_task8_kde_p30.png}
			\caption{Reservoir I}
		\end{subfigure}
		
%		\vspace{-0.2em} % Space between rows
		
		\begin{subfigure}[t]{0.4\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{chp_sogp/figures/TSNE_task11_kde_p30.png}
			\caption{Reservoir L}
		\end{subfigure}
		\hspace{0.05\columnwidth} % Reduced space between columns
		\begin{subfigure}[t]{0.4\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{chp_sogp/figures/TSNE_task20_kde_p30.png}
			\caption{Reservoir U}
		\end{subfigure}
		
		\vspace{-1.5em} % Space between the figures and the caption
	\end{figure}
\begin{itemize}
	\item Red and green points cluster near the data mode along the reservoir, capturing global information.
	\item The cyan point adjusts to capture task-specific details.
\end{itemize}


\end{frame}


\subsubsection{Performance Analysis}


\begin{frame}{One-day-ahead Models Forecasting}
	\centering
	\begin{figure}[htbp]
		\tiny
		\setlength\figurewidth{0.55\columnwidth} 
		\setlength\figureheight{0.5\columnwidth}
		
		\subfloat[Reservoir $A$]{\input{chp_sogp/figures/AMANI_forecasting}}
		\subfloat[Reservoir $I$]{\input{chp_sogp/figures/SAN LORENZO_forecasting}}
	\end{figure}
	\vspace{-1.5em}
	\begin{block}{}
	 The SVGP closely follows the time-series data and captures its stochastic nature through the predictive distribution.
	\end{block}
\end{frame}

\begin{frame}{Metrics achieved by LAR, LSTM, and SVGP}
	\vspace{-0.6em}
	\begin{figure}[htbp]
			\centering
			\tiny
			% Set dimensions for the first row images
			\setlength\figurewidth{0.38\columnwidth} 
			\setlength\figureheight{0.28\columnwidth}
			
			% First row: First image on the left, second image on the right (leaving a cell empty)
			\input{chp_sogp/figures/msll_lar}\hspace{18em} % First image without caption
			\input{chp_sogp/figures/msll_igp} \\ % Second image without caption
			
			% Second row: Three images filling the columns with captions
			\subfloat[LAR]{\input{chp_sogp/figures/mse_lar}}
			\subfloat[LSTM]{\input{chp_sogp/figures/mse_lstm}}
			\subfloat[SVGP]{\input{chp_sogp/figures/mse_igp}}
		\end{figure}
		\vspace{-1.3em}
	\begin{itemize}
		\item Longer horizons yield larger errors across all models.
		\item LAR and SVGP models significantly outperform LSTM models in all scenarios.
		\item SVGP models demonstrate lower errors and a slower increase in error with longer horizons compared to LAR models.
	\end{itemize}
\end{frame}


\begin{frame}{Performance Assessment}


	\begin{table}[htbp]
			\scriptsize
		\begin{tabular}{c p{1cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}}
			\toprule
			& \multicolumn{3}{c}{\textbf{MSE}} & \multicolumn{2}{c}{\textbf{MSLL}} & \multicolumn{2}{c}{\textbf{CRPS}} \\
			\cmidrule{2-8}
			\( \textbf{H} \) & \textbf{LAR} & \textbf{LSTM} & \textbf{SVGP} & \textbf{LAR} & \textbf{SVGP} & \textbf{LAR} & \textbf{SVGP} \\
			\midrule
			1  & 0.51 & 0.96 & 0.52 * & $-$0.39 & $-$0.53 & 0.34 & 0.32 \\
			2  & 0.63 & 1.01 & \textbf{0.61} * & $-$0.27 & \textbf{$-$0.42} & 0.39 & \textbf{0.36} \\
			3  & 0.68 & 1.02 & \textbf{0.65} * & $-$0.22 & \textbf{$-$0.38} & 0.41 & \textbf{0.38} \\
			4  & 0.72 & 1.03 & \textbf{0.69} * & $-$0.18 & \textbf{$-$0.34} & 0.42 & \textbf{0.39} \\
			5  & 0.74 & 1.06 & \textbf{0.71} * & $-$0.17 & \textbf{$-$0.33} & 0.43 & \textbf{0.40} \\
			6  & 0.76 & 1.07 & \textbf{0.72} * & $-$0.16 & \textbf{$-$0.32} & 0.44 & \textbf{0.40} \\
			7  & 0.76 & 1.11 & \textbf{0.74} * & $-$0.15 & \textbf{$-$0.31} & 0.44 & \textbf{0.41} \\
			14 & 0.83 & 1.12 & \textbf{0.79} * & $-$0.10 & \textbf{$-$0.27} & 0.46 & \textbf{0.43} \\
			21 & 0.88 & 1.08 & \textbf{0.83} * & $-$0.07 & \textbf{$-$0.23} & 0.48 & \textbf{0.45} \\
			30 & 0.91 & 1.06 & \textbf{0.88} * & $-$0.05 & \textbf{$-$0.20} & 0.49 & \textbf{0.46} \\
			\midrule
			Grand Average & 0.74 & 1.05 & \textbf{0.71} * & $-$0.18 & \textbf{$-$0.33} & 0.43 & \textbf{0.40} \\
			\bottomrule
		\end{tabular}
	
	\end{table}

	\begin{itemize}
	\item Bold and asterisk indicate a \emph{p}-value \(p < 1\%\) (LAR vs. SVGP, LSTM vs. SVGP).
	\item SVGP outperforms all models, except for LAR at \(H=1\), where linear dependence is stronger.
	\item As the horizon increases, SVGP effectively captures complex input-output relations, significantly outperforming the other models.
\end{itemize}
\end{frame}

%\subsubsection{Concluding Remark}
%\begin{frame}{To Conclude} 
%		\begin{block}{}
%			\justifying
%		The proposed methodology reduces computational complexity from cubic to linear, improving scalability for large datasets. 
%		\end{block}
%		
%		\begin{block}{}
%			\justifying
%		The optimal number of inducing points provides regularization, avoiding overfitting while capturing key data features. 
%		\end{block}
%		
%		\begin{block}{}
%			\justifying
%		The model strategically places shared inducing points, balancing task-specific and global dynamics to enhance streamflow forecasting. 
%		\end{block}
%		
%		\begin{block}{}
%			\justifying
%		Adaptive lengthscales allow the model to adjust to varying prediction horizons, improving robustness for multi-output tasks. 
%		\end{block}
%		
%		\begin{block}{}
%			\justifying
%		The SVGP model outperforms LAR and LSTM by better handling dynamics, providing uncertainty estimates, and showing slower error growth over long horizons.
%		\end{block}
%	 \end{frame}



