\section{Gaussian Process Regression: Bayesian Non-Parametric Model}

\subsection{Mathematical Framework}

\begin{frame}{Gaussian Process (GP) Framework}
	In a GP framework, the function $\bm{f}(\cdot)$ maps inputs $\bm{x}_n$ to outputs $\bm{y}_n$. Adding i.i.d. Gaussian noise $\bm{\epsilon}$, the model becomes:
	
	\[
	\bm{y}_n = \bm{f}(\bm{x}_n) + \bm{\epsilon}
	\]
	
	For test inputs $\mathbf{X}_*$, the joint distribution of training outputs $\mathbf{y}$ and test outputs $\mathbf{f}_*$ is:
	
	\[
	\left[ \begin{array}{c}
		\mathbf{y}\\
		\mathbf{f_*}\\
	\end{array}
	\right] \sim \mathcal{N} \left( \bm{0}, \left[
	\begin{array}{cc}
		\mathbf{K}_y & \mathbf{K}_* \\
		\mathbf{K}_*^\top & \mathbf{K}_{**}
	\end{array}\right] \right)
	\]
	
	The posterior distribution for test points is:
	
	\[
	\mathbf{f}_* | \mathbf{X}_*, \mathcal{D} \sim \mathcal{N}(\mathbf{K}_*^\top \mathbf{K}_y^{-1} \mathbf{y}, \mathbf{K}_{**} - \mathbf{K}_*^\top \mathbf{K}_y^{-1} \mathbf{K}_*)
	\]
	
	\begin{itemize}
		\item $\mathbf{K}_y = \mathbf{K} + \Sigma_\epsilon$, where $\mathbf{K} \in \mathbb{R}^{ND \times ND}$ is the covariance matrix for the train set and $\Sigma_\epsilon$ contains task-wise noise.
		
		\item $\mathbf{K}_{**} \in \mathbb{R}^{N_* D \times N_* D}$ is the covariance matrix for the test set.
		
		\item $\mathbf{K}_* \in \mathbb{R}^{ND \times N_* D}$ represents the cross-covariance matrix between the training and test points.
	\end{itemize}
	
\end{frame}


%\begin{frame}{The Marginal Log-likelihood}
%	\justifying
%	The prediction performance achieved by the conditional distribution is influenced by the selected parameter set $\bm{\theta}$ and the observation noise matrix $\Sigma_\epsilon$. These parameters are determined by maximizing the marginal log-likelihood, where the marginal likelihood $p(\mathbf{y})$ follows a Gaussian distribution:
%	
%	\[
%	p(\mathbf{y}) = \mathcal{N}(\mathbf{y} \mid \bm{0}, \mathbf{K}_y)
%	\]
%	
%	The optimization problem is defined as:
%	
%	\begin{equation*}\label{eq:sogp_nlml_opt}
%		\begin{aligned}
%			\{\bm{\theta}_{\text{opt}}, {\Sigma_\epsilon}_{\text{opt}}\} &= \underset{\bm{\theta}, \Sigma_\epsilon}{\arg\max} \quad \ln p(\mathbf{y}) \\
%			&= \underset{\bm{\theta}, \Sigma_\epsilon}{\arg\min} \quad \frac{1}{2} \mathbf{y}^\top \mathbf{K}_y^{-1} \mathbf{y} + \frac{1}{2} \ln \lvert \mathbf{K}_y \rvert + \frac{ND}{2} \ln 2\pi
%		\end{aligned}
%	\end{equation*}
%	
%	\vspace{3mm}
%	\justifying
%	However, the main challenge lies in the computational complexity of $\mathcal{O}(N^3D^3)$ and the storage demand of $\mathcal{O}(N^2D^2)$ due to the need to invert the matrix $\mathbf{K}_y$.
%\end{frame}
%
%\begin{frame}{Variational Inference and Sparse Variational GPs (SVGPs)}
%	We introduce $M \ll N$ inducing points $Z$, with inducing variables $\mathbf{u} \in \mathbb{R}^{MD}$ to reduce computational complexity. The joint distribution becomes:
%	\[
%	\begin{array}{rcl}
%		\left[ \begin{array}{c}
%			\mathbf{u}\\
%			\mathbf{f}\\
%		\end{array}
%		\right]
%		\sim
%		\mathcal{N} \left(
%		\begin{array}{c}
%			\mathbf{0}\\
%		\end{array},
%		\left[ \begin{array}{cc}
%			\mathbf{K}_{uu} & \mathbf{K}_{uf}\\
%			\mathbf{K}_{uf}^\top & \mathbf{K}\\
%		\end{array}
%		\right] \right)
%	\end{array}
%	\]
%	Where $\mathbf{K}_{uu} \in \mathbb{R}^{MD \times MD}$, and $\mathbf{K}_{uf} \in \mathbb{R}^{MD \times ND}$. The posterior distribution uses the variational approximation $q(\mathbf{u}) = \mathcal{N}(\boldsymbol{\mu}, \mathbf{S})$. Now we maximizing the Evidence Lower Bound (ELBO):
%	\begin{equation*}
%		\mathcal{L} = \sum_{d=1}^D \sum_{n=1}^N \mathbb{E}_{q(f_d(\mathbf{x}_n))}\{\ln p(y_{dn} \mid f_d(\mathbf{x}_n))\} - \sum_{d=1}^D \text{KL}\{q(\mathbf{u}_d) \parallel p(\mathbf{u}_d)\} \leq \ln p(\mathbf{y})
%	\end{equation*}
%	where $f_d(\mathbf{x}_n)$ represents the $d$-th latent function value at input $\mathbf{x}_n$, and $y_{dn}$ is the corresponding observed value. This reduces the complexity to $\mathcal{O}(NM^2D^3)$. For predictions at new points $\mathbf{x}_*$, we add noise in $\Sigma_\epsilon$ to $q(\mathbf{f}_*) = \int p(\mathbf{f}_* \mid \mathbf{u}) q(\mathbf{u}) d\mathbf{u}$.
%\end{frame}

\begin{frame}{Variational Inference for Scalability}

	The prediction performance is influenced by the selected parameter set \(\bm{\theta}\) and the matrix \(\Sigma_\epsilon\). These parameters are determined by maximizing the marginal log-likelihood:
	
	\begin{equation*}
		\{\bm{\theta}_{\text{opt}}, {\Sigma_\epsilon}_{\text{opt}}\} = \underset{\bm{\theta}, \Sigma_\epsilon}{\arg\max} \quad -\frac{1}{2} \mathbf{y}^\top \mathbf{K}_y^{-1} \mathbf{y} - \frac{1}{2} \ln \lvert \mathbf{K}_y \rvert - \frac{ND}{2} \ln 2\pi,
	\end{equation*}
	
	with complexity \textcolor{myNewColorB}{\(\mathcal{O}(N^3D^3)\)} due to the need to invert the matrix \(\mathbf{K}_y\).	For scalability, we introduce \(M \ll N\) inducing points \(Z\) with inducing variables $q(\mathbf{u}) = \mathcal{N}(\boldsymbol{\mu}, \mathbf{S})$, providing the following ELBO:
	\begin{equation*}
		\mathcal{L} = \sum_{d=1}^D \sum_{n=1}^N \mathbb{E}_{q(f_d(\mathbf{x}_n))}\{\ln p(y_{dn} \mid f_d(\mathbf{x}_n))\} - \sum_{d=1}^D \text{KL}\{q(\mathbf{u}_d) \parallel p(\mathbf{u}_d)\},
	\end{equation*}
	where $f_d(\mathbf{x}_n)$ represents the $d$-th latent function value at input $\mathbf{x}_n$, and $y_{dn}$ is the corresponding observed value. This reduces the complexity to \textcolor{myNewColorB}{$\mathcal{O}(NM^2D^3)$}.
\end{frame}


\begin{frame}{Model Setup}
	The GP covariance is factorized into two kernels: $k_{\mathcal{X}}$ for input correlations and $k_{D}$ for task correlations:
	\[
	k\left((\bm{x}, d), (\bm{x}', d')\right) = k_{\mathcal{X}}\left(\bm{x}, \bm{x}' \mid \bm{\Theta}_d \right) k_{D}\left(d, d' \mid \sigma_d \right),
	\]
	with:
	\[
	k_{\mathcal{X}}\left(\bm{x}, \bm{x}'\right) = \exp\left(-\frac{1}{2}(\bm{x} - \bm{x}')^\top \bm{\Theta}_d^{-2} (\bm{x} - \bm{x}')\right),
	\]
	\[
	k_{D}(d, d') = \sigma^2_d \delta_{d, d'},
	\]
	where $\delta_{d, d'}$ is the Kronecker delta, $\bm{\Theta}_d=\text{diag}\{\Delta_{dl}\}_{l=1}^{DT}$ is the lengthscale matrix, and $\sigma^2_d$ is the output scale. This reduces complexity to \textcolor{myNewColorB}{$\mathcal{O}(NM^2D)$} by avoiding explicit task correlations.
\end{frame}



\subsection{Results and Discussions}

\begin{frame}{Grid search average values for tuning $T$ and $M$}
	\begin{figure}[htbp]
		\setlength\figurewidth{0.5\textwidth} 
		\setlength\figureheight{0.5\textwidth}
		\subfloat[MSE ($\times10^{-1}$)]{\input{chp_sogp/figures/gs_mse}}\hspace{-1em}
		\subfloat[MSLL ($\times10^{-2}$)]{\input{chp_sogp/figures/gs_msll}}
	\end{figure}
	\vspace{-2em}
	\begin{block}{}
		The error increases with larger values of \( T \). Smaller values of \( M \) are more sensitive to initialization, while larger values of \( M \) tend to overfit. The optimal parameters are \( M = 64 \) and \( T = 1 \).
	\end{block}
\end{frame}


\begin{frame}{Reservoir-Wise Output Scales and Noise Variance}
	
	\begin{figure}[htbp]
		 \centering
		\tiny
	 	\setlength\figurewidth{0.48\textwidth} 
		\setlength\figureheight{0.48\textwidth}
		\subfloat[Output scales $\sigma^2_d$]{\input{chp_sogp/figures/output_scale_matrix}}
		\subfloat[Noise variances $\Sigma_\epsilon$]{\input{chp_sogp/figures/noise_variance_matrix}}
	\end{figure}
	\vspace{-2.0em}
	\begin{block}{}
	Longer horizons typically exhibit smaller output scales and higher noise variances, indicating weaker correlations and more complex tasks.
	\end{block}
\end{frame}


\begin{frame}{Reservoir-Wise Lengthscales}
	\begin{figure}[htbp]
		\centering
		\tiny
		\setlength\figurewidth{0.4\columnwidth} 
		\setlength\figureheight{0.42\columnwidth}
		\subfloat[$H=1$]{\input{chp_sogp/figures/lengthscalematrix1}}\hspace{-1.3em}
		\subfloat[$H=14$]{\input{chp_sogp/figures/lengthscalematrix14}}\hspace{-1.3em}
		\subfloat[$H=30$]{\input{chp_sogp/figures/lengthscalematrix30}}
	\end{figure}
	\vspace{-2.0em}
	\begin{block}{}
	As the horizon increases, main diagonal lengthscales lose relevance, while off-diagonal ones gain importance.
	\end{block}
\end{frame}

\begin{frame}{SVGP t-SNE Latent Mapping Space}
	\begin{figure}[htbp]
		\centering
		\tiny
		\begin{subfigure}[t]{0.4\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{chp_sogp/figures/TSNE_task4_kde_p30.png}
			\caption{Reservoir E}
		\end{subfigure}
		\hspace{0.05\columnwidth} % Reduced space between columns
		\begin{subfigure}[t]{0.4\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{chp_sogp/figures/TSNE_task8_kde_p30.png}
			\caption{Reservoir I}
		\end{subfigure}
		
%		\vspace{-0.2em} % Space between rows
		
		\begin{subfigure}[t]{0.4\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{chp_sogp/figures/TSNE_task11_kde_p30.png}
			\caption{Reservoir L}
		\end{subfigure}
		\hspace{0.05\columnwidth} % Reduced space between columns
		\begin{subfigure}[t]{0.4\columnwidth}
			\centering
			\includegraphics[width=\columnwidth]{chp_sogp/figures/TSNE_task20_kde_p30.png}
			\caption{Reservoir U}
		\end{subfigure}
		
		\vspace{-1.5em} % Space between the figures and the caption
	\end{figure}
	\begin{block}{}
	 The shared inducing points allow for the capturing of task-wise, and global information about the streamflow dynamics.
	\end{block}
\end{frame}


\subsubsection{Performance Analysis}


\begin{frame}{Models Forecasting}
	\centering
	\begin{figure}[htbp]
		\tiny
		\setlength\figurewidth{0.55\columnwidth} 
		\setlength\figureheight{0.5\columnwidth}
		
		\subfloat[$A$.]{\input{chp_sogp/figures/AMANI_forecasting}}
		\subfloat[$I$.]{\input{chp_sogp/figures/SAN LORENZO_forecasting}}
	\end{figure}
	\vspace{-1.5em}
	\begin{block}{}
	One-day-ahead model predictions for two reservoirs. The SVGP adapts better to time-series data and captures its stochastic nature through the predictive distribution.
	\end{block}
\end{frame}


\begin{frame}{MSE Scores}
	\begin{figure}[htbp]
		%\centering
		\setlength\figurewidth{0.37\columnwidth} 
		\setlength\figureheight{0.37\columnwidth}
		\subfloat[LAR.]{\input{chp_sogp/figures/mse_lar}}\hspace{-1.5em}
		\subfloat[LSTM.]{\input{chp_sogp/figures/mse_lstm}}\hspace{-1.5em}
		\subfloat[SVGP.]{\input{chp_sogp/figures/mse_igp}}
	\end{figure}
	\vspace{-1.5em}
	\begin{block}{}
	MSE achieved by LAR, LSTM, and SVGP forecasting models for each horizon and reservoir. The LAR and SVGP models significantly outperform the LSTM models across all scenarios.
	\end{block}
\end{frame}

\begin{frame}{MSLL Score}
	\begin{figure}[htbp]
		%\centering
		\setlength\figurewidth{0.5\columnwidth} 
		\setlength\figureheight{0.42\columnwidth}
		\subfloat[LAR.]{\input{chp_sogp/figures/msll_lar}}\hspace{-1.5em}
		\subfloat[SVGP.]{\input{chp_sogp/figures/msll_igp}}
	\end{figure}
	\vspace{-1.8em}
	\begin{block}{}
	Achieved MSLL for LAR and SVGP models across horizons and reservoirs. Longer horizons yield larger errors. The SVGP models exhibits lower error and a slower error increase with the horizon compared to the LAR models.
	\end{block}
\end{frame}

\begin{frame}{Performance t-test}
	\begin{block}{}
	Performance metrics for LAR, LSTM, and SVGP across horizons $H$. Bold and asterisk indicate a \emph{p}-value $p<1\%$ (LAR vs. SVGP, LSTM vs. SVGP). SVGP outperforms all models, except LAR at $H=1$, where linear dependence is stronger. As horizon increases, SVGP captures complex input-output relations, significantly outperforming the other models.
	\end{block}
	\scriptsize
	\begin{table}[htbp]
		\begin{tabular}{c p{1cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}}
			\toprule
			& \multicolumn{3}{c}{\textbf{MSE}} & \multicolumn{2}{c}{\textbf{MSLL}} & \multicolumn{2}{c}{\textbf{CRPS}} \\
			\cmidrule{2-8}
			\( \textbf{H} \) & \textbf{LAR} & \textbf{LSTM} & \textbf{SVGP} & \textbf{LAR} & \textbf{SVGP} & \textbf{LAR} & \textbf{SVGP} \\
			\midrule
			1  & 0.51 & 0.96 & 0.52 * & $-$0.39 & $-$0.53 & 0.34 & 0.32 \\
			2  & 0.63 & 1.01 & \textbf{0.61} * & $-$0.27 & \textbf{$-$0.42} & 0.39 & \textbf{0.36} \\
			3  & 0.68 & 1.02 & \textbf{0.65} * & $-$0.22 & \textbf{$-$0.38} & 0.41 & \textbf{0.38} \\
			4  & 0.72 & 1.03 & \textbf{0.69} * & $-$0.18 & \textbf{$-$0.34} & 0.42 & \textbf{0.39} \\
			5  & 0.74 & 1.06 & \textbf{0.71} * & $-$0.17 & \textbf{$-$0.33} & 0.43 & \textbf{0.40} \\
			6  & 0.76 & 1.07 & \textbf{0.72} * & $-$0.16 & \textbf{$-$0.32} & 0.44 & \textbf{0.40} \\
			7  & 0.76 & 1.11 & \textbf{0.74} * & $-$0.15 & \textbf{$-$0.31} & 0.44 & \textbf{0.41} \\
			14 & 0.83 & 1.12 & \textbf{0.79} * & $-$0.10 & \textbf{$-$0.27} & 0.46 & \textbf{0.43} \\
			21 & 0.88 & 1.08 & \textbf{0.83} * & $-$0.07 & \textbf{$-$0.23} & 0.48 & \textbf{0.45} \\
			30 & 0.91 & 1.06 & \textbf{0.88} * & $-$0.05 & \textbf{$-$0.20} & 0.49 & \textbf{0.46} \\
			\midrule
			Grand Average & 0.74 & 1.05 & \textbf{0.71} * & $-$0.18 & \textbf{$-$0.33} & 0.43 & \textbf{0.40} \\
			\bottomrule
		\end{tabular}
		
	\end{table}
\end{frame}

\subsubsection{Concluding Remark}
\begin{frame}{To Conclude} 
		\begin{block}{}
			\justifying
		The proposed methodology reduces computational complexity from cubic to linear, improving scalability for large datasets. 
		\end{block}
		
		\begin{block}{}
			\justifying
		The optimal number of inducing points provides regularization, avoiding overfitting while capturing key data features. 
		\end{block}
		
		\begin{block}{}
			\justifying
		The model strategically places shared inducing points, balancing task-specific and global dynamics to enhance streamflow forecasting. 
		\end{block}
		
		\begin{block}{}
			\justifying
		Adaptive lengthscales allow the model to adjust to varying prediction horizons, improving robustness for multi-output tasks. 
		\end{block}
		
		\begin{block}{}
			\justifying
		The SVGP model outperforms LAR and LSTM by better handling dynamics, providing uncertainty estimates, and showing slower error growth over long horizons.
		\end{block}
	 \end{frame}



