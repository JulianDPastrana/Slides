@article{Chang2025,
	abstract = {Time series data in real-world applications such as healthcare, climate modeling, and finance are often irregular, multimodal, and messy, with varying sampling rates, asynchronous modalities, and pervasive missingness. However, existing benchmarks typically assume clean, regularly sampled, unimodal data, creating a significant gap between research and real-world deployment. We introduce Time-IMM, a dataset specifically designed to capture cause-driven irregularity in multimodal multivariate time series. Time-IMM represents nine distinct types of time series irregularity, categorized into trigger-based, constraint-based, and artifact-based mechanisms. Complementing the dataset, we introduce IMM-TSF, a benchmark library for forecasting on irregular multimodal time series, enabling asynchronous integration and realistic evaluation. IMM-TSF includes specialized fusion modules, including a timestamp-to-text fusion module and a multimodality fusion module, which support both recency-aware averaging and attention-based integration strategies. Empirical results demonstrate that explicitly modeling multimodality on irregular time series data leads to substantial gains in forecasting performance. Time-IMM and IMM-TSF provide a foundation for advancing time series analysis under real-world conditions. The dataset is publicly available at https://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the benchmark library can be accessed at https://anonymous.4open.science/r/IMMTSF_NeurIPS2025.},
	author = {Ching Chang and Jeehyun Hwang and Yidan Shi and Haixin Wang and Wen-Chih Peng and Tien-Fu Chen and Wei Wang},
	month = {6},
	title = {Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series},
	url = {http://arxiv.org/abs/2506.10412},
	year = {2025}
}

@misc{Vos2023,
	abstract = {Introduction: Wearable sensors have shown promise as a non-intrusive method for collecting biomarkers that may correlate with levels of elevated stress. Stressors cause a variety of biological responses, and these physiological reactions can be measured using biomarkers including Heart Rate Variability (HRV), Electrodermal Activity (EDA) and Heart Rate (HR) that represent the stress response from the Hypothalamic-Pituitary-Adrenal (HPA) axis, the Autonomic Nervous System (ANS), and the immune system. While Cortisol response magnitude remains the gold standard indicator for stress assessment [1], recent advances in wearable technologies have resulted in the availability of a number of consumer devices capable of recording HRV, EDA and HR sensor biomarkers, amongst other signals. At the same time, researchers have been applying machine learning techniques to the recorded biomarkers in order to build models that may be able to predict elevated levels of stress. Objective: The aim of this review is to provide an overview of machine learning techniques utilized in prior research with a specific focus on model generalization when using these public datasets as training data. We also shed light on the challenges and opportunities that machine learning-enabled stress monitoring and detection face. Methods: This study reviewed published works contributing and/or using public datasets designed for detecting stress and their associated machine learning methods. The electronic databases of Google Scholar, Crossref, DOAJ and PubMed were searched for relevant articles and a total of 33 articles were identified and included in the final analysis. The reviewed works were synthesized into three categories of publicly available stress datasets, machine learning techniques applied using those, and future research directions. For the machine learning studies reviewed, we provide an analysis of their approach to results validation and model generalization. The quality assessment of the included studies was conducted in accordance with the IJMEDI checklist [2]. Results: A number of public datasets were identified that are labeled for stress detection. These datasets were most commonly produced from sensor biomarker data recorded using the Empatica E4 device, a well-studied, medical-grade wrist-worn wearable that provides sensor biomarkers most notable to correlate with elevated levels of stress. Most of the reviewed datasets contain less than twenty-four hours of data, and the varied experimental conditions and labeling methodologies potentially limit their ability to generalize for unseen data. In addition, we discuss that previous works show shortcomings in areas such as their labeling protocols, lack of statistical power, validity of stress biomarkers, and model generalization ability. Conclusion: Health tracking and monitoring using wearable devices is growing in popularity, while the generalization of existing machine learning models still requires further study, and research in this area will continue to provide improvements as newer and more substantial datasets become available.},
	author = {Gideon Vos and Kelly Trinh and Zoltan Sarnyai and Mostafa Rahimi Azghadi},
	doi = {10.1016/j.ijmedinf.2023.105026},
	issn = {18728243},
	journal = {International Journal of Medical Informatics},
	keywords = {Generalization,Machine learning,Stress,Wearable sensor},
	month = {5},
	pmid = {36893657},
	publisher = {Elsevier Ireland Ltd},
	title = {Generalizable machine learning for stress monitoring from wearable devices: A systematic literature review},
	volume = {173},
	year = {2023}
}

@article{Zhu2023,
	abstract = {Stress is an inevitable part of modern life. While stress can negatively impact a person's life and health, positive and under-controlled stress can also enable people to generate creative solutions to problems encountered in their daily lives. Although it is hard to eliminate stress, we can learn to monitor and control its physical and psychological effects. It is essential to provide feasible and immediate solutions for more mental health counselling and support programs to help people relieve stress and improve their mental health. Popular wearable devices, such as smartwatches with several sensing capabilities, including physiological signal monitoring, can alleviate the problem. This work investigates the feasibility of using wrist-based electrodermal activity (EDA) signals collected from wearable devices to predict people's stress status and identify possible factors impacting stress classification accuracy. We use data collected from wrist-worn devices to examine the binary classification discriminating stress from non-stress. For efficient classification, five machine learning-based classifiers were examined. We explore the classification performance on four available EDA databases under different feature selections. According to the results, Support Vector Machine (SVM) outperforms the other machine learning approaches with an accuracy of 92.9 for stress prediction. Additionally, when the subject classification included gender information, the performance analysis showed significant differences between males and females. We further examine a multimodal approach for stress classifications. The results indicate that wearable devices with EDA sensors have a great potential to provide helpful insight for improved mental health monitoring.},
	author = {Lili Zhu and Petros Spachos and Pai Chet Ng and Yuanhao Yu and Yang Wang and Konstantinos Plataniotis and Dimitrios Hatzinakos},
	doi = {10.1109/JBHI.2023.3239305},
	issn = {21682208},
	issue = {5},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	keywords = {EDA,Stress measurement,emotion recognition,k-nearest neighbors,logistic regression,naive bayes,random forest,smartwatches,support vector machines,wearable sensors,wrist-worn wearable device},
	month = {5},
	pages = {2155-2165},
	pmid = {37022004},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	title = {Stress Detection Through Wrist-Based Electrodermal Activity Monitoring and Machine Learning},
	volume = {27},
	year = {2023}
}


@article{Yang2023,
	abstract = {With the rapid development of mobile and wearable devices, it is increasingly possible to access users' affective data in a more unobtrusive manner. On this basis, researchers have proposed various systems to recognize user's emotional states. However, most of these studies rely on traditional machine learning techniques and a limited number of signals, leading to systems that either do not generalize well or would frequently lack sufficient information for emotion detection in realistic scenarios. In this paper, we propose a novel attention-based LSTM system that uses a combination of sensors from a smartphone (front camera, microphone, touch panel) and a wristband (photoplethysmography, electrodermal activity, and infrared thermopile sensor) to accurately determine user's emotional states. We evaluated the proposed system by conducting a user study with 45 participants. Using collected behavioral (facial expression, speech, keystroke) and physiological (blood volume, electrodermal activity, skin temperature) affective responses induced by visual stimuli, our system was able to achieve an average accuracy of 89.2 percent for binary positive and negative emotion classification under leave-one-participant-out cross-validation. Furthermore, we investigated the effectiveness of different combinations of data signals to cover different scenarios of signal availability.},
	author = {Kangning Yang and Chaofan Wang and Yue Gu and Zhanna Sarsenbayeva and Benjamin Tag and Tilman Dingler and Greg Wadley and Jorge Goncalves},
	doi = {10.1109/TAFFC.2021.3100868},
	issn = {19493045},
	issue = {2},
	journal = {IEEE Transactions on Affective Computing},
	keywords = {Emotion recognition,attention-based LSTM,behavioral signals,mobile and wearable devices,physiological signals},
	month = {4},
	pages = {1082-1097},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	title = {Behavioral and Physiological Signals-Based Deep Multimodal Approach for Mobile Emotion Recognition},
	volume = {14},
	year = {2023}
}

% Multimodalidad
@misc{Baltrusaitis2019,
	abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
	author = {Tadas Baltrusaitis and Chaitanya Ahuja and Louis Philippe Morency},
	doi = {10.1109/TPAMI.2018.2798607},
	issn = {19393539},
	issue = {2},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Multimodal,introductory,machine learning,survey},
	month = {2},
	pages = {423-443},
	pmid = {29994351},
	publisher = {IEEE Computer Society},
	title = {Multimodal Machine Learning: A Survey and Taxonomy},
	volume = {41},
	year = {2019}
}

@article{Liang2024,
	abstract = {Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this article is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining three key principles of modality heterogeneity, connections, and interactions that have driven subsequent innovations, and propose a taxonomy of six core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.},
	author = {Paul Pu Liang and Amir Zadeh and Louis Philippe Morency},
	doi = {10.1145/3656580},
	issn = {15577341},
	issue = {10},
	journal = {ACM Computing Surveys},
	keywords = {Multimodal machine learning,data heterogeneity,feature interactions,language and vision,multimedia,representation learning},
	month = {6},
	publisher = {Association for Computing Machinery},
	title = {Foundations \& Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions},
	volume = {56},
	year = {2024}
}


@article{Venugopalan2021,
	abstract = {Most current Alzheimer’s disease (AD) and mild cognitive disorders (MCI) studies use single data modality to make predictions such as AD stages. The fusion of multiple data modalities can provide a holistic view of AD staging analysis. Thus, we use deep learning (DL) to integrally analyze imaging (magnetic resonance imaging (MRI)), genetic (single nucleotide polymorphisms (SNPs)), and clinical test data to classify patients into AD, MCI, and controls (CN). We use stacked denoising auto-encoders to extract features from clinical and genetic data, and use 3D-convolutional neural networks (CNNs) for imaging data. We also develop a novel data interpretation method to identify top-performing features learned by the deep-models with clustering and perturbation analysis. Using Alzheimer’s disease neuroimaging initiative (ADNI) dataset, we demonstrate that deep models outperform shallow models, including support vector machines, decision trees, random forests, and k-nearest neighbors. In addition, we demonstrate that integrating multi-modality data outperforms single modality models in terms of accuracy, precision, recall, and meanF1 scores. Our models have identified hippocampus, amygdala brain areas, and the Rey Auditory Verbal Learning Test (RAVLT) as top distinguished features, which are consistent with the known AD literature.},
	author = {Janani Venugopalan and Li Tong and Hamid Reza Hassanzadeh and May D. Wang},
	doi = {10.1038/s41598-020-74399-w},
	issn = {20452322},
	issue = {1},
	journal = {Scientific Reports},
	month = {12},
	pmid = {33547343},
	publisher = {Nature Research},
	title = {Multimodal deep learning models for early detection of Alzheimer’s disease stage},
	volume = {11},
	year = {2021}
}


@ARTICLE{Wan2020743,
	author = {Wan, Shaohua and Qi, Lianyong and Xu, Xiaolong and Tong, Chao and Gu, Zonghua},
	title = {Deep Learning Models for Real-time Human Activity Recognition with Smartphones},
	year = {2020},
	journal = {Mobile Networks and Applications},
	volume = {25},
	number = {2},
	pages = {743 – 755},
	doi = {10.1007/s11036-019-01445-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077197854&doi=10.1007%2fs11036-019-01445-x&partnerID=40&md5=71b473bfab3da580128e9384cc281aef},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 470}
}

@ARTICLE{Yang20231082,
	author = {Yang, Kangning and Wang, Chaofan and Gu, Yue and Sarsenbayeva, Zhanna and Tag, Benjamin and Dingler, Tilman and Wadley, Greg and Goncalves, Jorge},
	title = {Behavioral and Physiological Signals-Based Deep Multimodal Approach for Mobile Emotion Recognition},
	year = {2023},
	journal = {IEEE Transactions on Affective Computing},
	volume = {14},
	number = {2},
	pages = {1082 – 1097},
	doi = {10.1109/TAFFC.2021.3100868},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111606838&doi=10.1109%2fTAFFC.2021.3100868&partnerID=40&md5=5c974c3c7ad775de256eaf0806008b86},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 45}
}

@ARTICLE{Zhang20211138,
	author = {Zhang, Guangyi and Etemad, Ali},
	title = {Capsule Attention for Multimodal EEG-EOG Representation Learning with Application to Driver Vigilance Estimation},
	year = {2021},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	volume = {29},
	pages = {1138 – 1149},
	doi = {10.1109/TNSRE.2021.3089594},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109115570&doi=10.1109%2fTNSRE.2021.3089594&partnerID=40&md5=afcfafb12199974aa7fd91c0c8464527},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 65; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Li20213323,
	author = {Li, Qing and Tan, Jinghua and Wang, Jun and Chen, Hsinchun},
	title = {A Multimodal Event-Driven LSTM Model for Stock Prediction Using Online News},
	year = {2021},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	volume = {33},
	number = {10},
	pages = {3323 – 3337},
	doi = {10.1109/TKDE.2020.2968894},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114961859&doi=10.1109%2fTKDE.2020.2968894&partnerID=40&md5=ee7dd86c9764c664e3ccb3deb10fb059},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 122; All Open Access, Bronze Open Access}
}