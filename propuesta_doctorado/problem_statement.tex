
\section{Planteamiento del problema y pregunta de investigación}

Los trastornos mentales afectan la cognición, el comportamiento y las emociones de millones de personas en el mundo. Se estima que alrededor de 350 millones de individuos padecen algún trastorno mental \cite{Dehghan-Bonari2023}. Dentro de estos trastornos, el Trastorno por Déficit de Atención e Hiperactividad (TDAH) representa un desafío significativo, afectando aproximadamente al 5\% de la población infantil y adolescente, y al 2.5\% de los adultos, convirtiéndolo en uno de los desordenes neuronales más frecuentes en la infancia y adolescencia, además es una de las principales causas de tratamiento psicológico y psiquiátrico \cite{Salari2023}. Este trastorno no solo dificulta el rendimiento académico y laboral \cite{Ayano2020}, sino que también aumenta el riesgo de presentar otros problemas psiquiátricos, conductas delictivas y adicciones \cite{Faraone2015}.

La detección e intervención temprana del TDAH es esencial para brindar un tratamiento oportuno y efectivo \cite{Kivumbi2019}. Sin embargo, las técnicas diagnósticas actuales presentan limitaciones importantes: muchas pruebas requieren un seguimiento prolongado \cite{Zhou2015-bg}, pueden estar sujetas a interpretaciones subjetivas \cite{LOHANI2023111689} y muchos criterios resultan ineficientes para el diagnostico en adultos \cite{Sibley21042021}. Además, el acceso a un tratamiento clínico y seguimiento es frecuentemente restringido debido a la falta de recursos o la escasez de especialistas \cite{Asherson2022,Pallanti2020}.

El avance en el desarrollo de modelos de aprendizaje automático ha permitido la creación de herramientas que apoyan el diagnóstico de diversas enfermedades, incluido el TDAH \cite{article}. Generalmente, estos modelos utilizan señales electroencefalográficas (EEG) para clasificar registros como pertenecientes a individuos con o sin la enfermedad, mediante un enfoque de clasificación binaria. No obstante, la naturaleza no estacionaria y la complejidad inherente a las señales EEG complican su análisis utilizando modelos tradicionales \cite{KHARE2023106676,Loh2024}.

El uso de técnicas de aprendizaje profundo se presenta como una alternativa prometedora, ya que permite extraer características abstractas de las señales EEG mediante una gran cantidad de parámetros \cite{Jahani2024,Garcia-Argibay2023,Moghaddari2020}. Sin embargo, el entrenamiento de estos modelos requiere grandes volúmenes de datos etiquetados en un marco supervisado, lo que en muchos casos resulta costoso o incluso inviable, además de en muchos casos carecen de inalterabilidad para mejorar su implementación \cite{emam2021statedatacomputervision,healthcare11030285}.

Para superar esta limitación, han surgido los modelos fundacionales, que permiten preentrenar con conjuntos de datos no etiquetados y, posteriormente, ajustar el modelo para tareas específicas utilizando una cantidad significativamente menor de datos etiquetados \cite{Mathew2024,Abbaspourazad2023}. Este enfoque aprovecha la estructura subyacente presente en datos sin clasificar, permitiendo que el modelo aprenda representaciones generales robustas. Gracias a este preentrenamiento, el modelo puede transferir conocimientos a tareas concretas, lo que reduce la dependencia de grandes volúmenes de datos anotados y agiliza el proceso de adaptación a nuevos escenarios. 

En el ámbito clínico es crucial no solo obtener una predicción diagnóstica, sino también disponer de una medida de la incertidumbre asociada a dicha predicción. En este sentido, los procesos gaussianos ofrecen una solución al generar distribuciones de probabilidad en lugar de valores deterministas, lo que facilita el modelado de funciones complejas \cite{GUTIERREZBECKER2018246,Jahani2024}. Sin embargo, la integración de procesos gaussianos con modelos fundacionales aún es un área poco explorada y con un gran potencial de mejora.

Un aspecto relevante en el preentrenamiento es la variabilidad inherente a los conjuntos de datos no etiquetados. Estas bases de datos pueden diferir en los estándares de adquisición, lo que se manifiesta en la presencia de valores ausentes, variaciones en la tasa de muestreo, diferencias en el número de canales e incluso problemas de generalización al cambiar de sujetos o sesiones (conocido como Dataset Shift) \cite{APICELLA2024128354}. Descartar estos datos supondría la pérdida de información potencialmente valiosa; por ello, es fundamental desarrollar un modelo fundacional capaz de gestionar eficazmente dichas inconsistencias y maximizar el aprovechamiento de los datos disponibles.

\textbf{Pregunta de investigación:}  
¿Cómo desarrollar un modelo fundacional estocástico para la clasificación de señales biológicas que integre procesos gaussianos, para el soporte de la detección del TDAH mediante el análisis de señales EEG, considerando la variabilidad e inconsistencias presentes en los conjuntos de datos?
