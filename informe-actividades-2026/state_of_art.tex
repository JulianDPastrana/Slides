\section{State of Art}
Wearable devices enable non-intrusive measurement of physiological biomarkers that correlate with stress levels, emotional states, and other biological responses. Those measurements often include heart rate Variability (HRV), Electrodermal Activity (ADA), Heart Rate (HR), and three-axis acceleration (ACC) \cite{Vos2023}. Advances in machine learning have allowed us to predict emotional states from these biomarkers, reflecting a shift toward recognizing mental well-being as an integral component of human health.

Authors in \cite{Zhu2023} evaluate a set of traditional machine learning algorithms to predict people's stress based on EDA activity, including K-Nearest Neighbor, Support Vector Machine (SVM), Naive Bayes, Logistic Regression, and Random Forest. They trained models on both statistical features and raw sensor readings, finding that SVM achieved the highest accuracy, although performance varied inconsistently between feature-based and raw-data approaches.

Despite their utility, shallow models often lack expressiveness and capacity to generalize well \cite{Yang2023}. Moreover, features often rely on statistics, forgetting sequential dependencies in the data. A closer overview dives us into a multi-modality scenario, where signals are sampled at different frequencies, introducing additional challenges for feature extraction and fusion.

Deep learning approaches address these limitations by automatically leveraging data structures as time dependencies for sequential recordings or spatial patterns for images through feature representation from multiple data entities. However, a key challenge lies in effectively combining heterogeneous data sources \cite{Baltrusaitis2019,Liang2024}. 

The work developed by \cite{Venugopalan2021} demonstrated the power of multimodal fusion by integrating autoencoders for genetic data with 3D CNNs for imaging, outperforming shallow and single‑modality baselines. In the domain of physiological sensing, \cite{Wan2020743} proposed a CNN-based feature extractor for time-series sensor data, while \cite{Yang20231082} developed an attention-based LSTM framework to fuse smartphone and wearable signals for emotion recognition. Prior studies by \cite{Zhang20211138,Li20213323} further highlight the effectiveness of LSTM architectures in modeling inter‑participant variability and integrating multiple modalities.

%Building on these findings, our approach leverages LSTM deep learning models to handle multimodal physiological signals sampled at disparate frequencies. By learning unified temporal representations, LSTMs can capture both short‑term fluctuations and long‑term trends in the data. Multimodal fusion within this framework enables the model to exploit complementary information across different biomarkers, achieving robustness and accuracy in emotion detection from wearable devices.