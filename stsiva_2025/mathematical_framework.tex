\section{Mathematical Framework}

\begin{frame}{Problem Setting}

Consider a vector time-series collecting sequential data observed across $P$ outputs at some time instant \( t \), denoted as \( \boldsymbol{v}_t \subseteq \mathbb{R}^{P}\). The flattening of \( T \) sequential observations simultaneously yields an input vector \( \boldsymbol{x} \) belonging to some input space \( \mathcal{X} \), that is
\[
\boldsymbol{x}= [ \boldsymbol{v}_{t-1}^\top, \boldsymbol{v}_{t-2}^\top, \cdots, \boldsymbol{v}_{t-T}^\top  ]^\top,
\]
with \( {\mathcal{X}} \subseteq \mathbb{R}^{L} \), \( L = PT \), and \( T \) as the model order. The forecasting task aims to predict the next \(H\) sequential values, yielding the output target \( \boldsymbol{y} \in {\mathcal{Y}} \) as
\[
\boldsymbol{y} = [ \boldsymbol{v}_{t}^\top, \boldsymbol{v}_{t+1}^\top, \cdots, \boldsymbol{v}_{t+H-1}^\top  ]^\top,
\] 
being the output space \( {\mathcal{Y}} \subseteq \mathbb{R}^{D} \), \( D = PH \), and \( H \) the model horizon. The above formulation enables the model to leverage sequential data for accurate future predictions. Gathering \( N \) input-output i.i.d. observation pairs produces the training dataset as \( {\mathcal{D}} = \{\boldsymbol{x}_n, \boldsymbol{y}_n\}_{n=1}^N = \{ \boldsymbol{X}, \boldsymbol{Y}\} \).

\end{frame}

\begin{frame}{The Chained Model}

For data in $\{\mathcal{X},\mathcal{Y}\}$, the likelihood function assumes the distribution over \(  \boldsymbol{Y} \) as the product of \( D \) conditionally independent distributions, one by output as follows:
%\(  \boldsymbol{y}_n \) as the product of \( D \) conditionally independent distributions, one by output as follows:
\begin{equation}\label{eq:likelihood_funciton}
	p(\boldsymbol{Y} \mid \boldsymbol{\theta}(\boldsymbol{X})) = \prod_{n=1}^N \prod_{d=1}^D p\left(y_{n,d} \mid \boldsymbol{\theta}_d(\boldsymbol{x}_n)\right),
\end{equation}
where \( \boldsymbol{\theta}(\boldsymbol{X}) = \{ \boldsymbol{\theta}_d(\boldsymbol{x}_n \}_{n=1,d=1}^{N, D} \), with \( \boldsymbol{\theta}_d(\boldsymbol{x}) \subseteq \mathbb{R}^{J_d} \) as a vector containing \( J_d \) parameter for \( d \)-th output distribution. Since each element of \( \boldsymbol{\theta}_d(\boldsymbol{x}) \), denoted as \( \theta_{d,j}(\boldsymbol{x}) \), is restricted to some subset of \( \mathbb{R} \), we model \( \theta_{d,j}(\boldsymbol{x}) = h_{d,j}(f_{d,j}(\boldsymbol{x})) \) as a transformation of an unrestricted latent variable \( f_{d,j}(\boldsymbol{x}) \) via a link function \( h_{d,j} \).

\end{frame}

\begin{frame}{The LMC Model}
	
Then, the task reduces to model each \(f_{d,j}\) and plug-in it to \cref{eq:likelihood_funciton}. Consider a set of \( Q \) independent GPs \( \{ g_q \}_{q=1}^Q \) over the input space such that \( g_q(\boldsymbol{x}) \sim \mathcal{GP}(0, k_q(\boldsymbol{x}, \boldsymbol{x}'))\) with kernel parameters \( \Phi_q \) that will be linearly weighed via \( a_{(d,j),q} \in \mathbb{R} \) coefficients to generate \( f_{d,j} \) by:
\begin{equation}\label{eq:lmc_process}
	f_{d,j}(\boldsymbol{x}) = \sum_{q=1}^Q a_{(d,j),q} g_{q}(\boldsymbol{x}).
\end{equation}

Particularly, LMC proposes the cross-covariance function for the latent variable \( f_{d,j} \) as in \cref{eq:cross-covariance_function}. Such a covariance shares output dependencies via a mixing process, allowing each \( f_{d,j} \) to be composed as a set of features granted by \( g_q \) through coefficients \( a_{(d,j),q} \).
\begin{equation}
	\begin{split}\label{eq:cross-covariance_function}
		k_{\boldsymbol{f}_{d,j}, \boldsymbol{f}_{d',j'}}(\boldsymbol{x}, \boldsymbol{x}') &= \text{cov}\{f_{d,j}(\boldsymbol{x}), f_{d',j'}(\boldsymbol{x}')\}\\
		&=\sum_{q=1}^Q a_{(d,j),q}a_{(d',j'),q} k_{q}(\boldsymbol{x}, \boldsymbol{x}').
	\end{split}
\end{equation}

\end{frame}