\section{Experimental Setup}

\begin{frame}{Dataset}

\begin{block}{}
	We use daily time series of streamflow contributions recorded from 23 Colombian reservoirs. The dataset contains 4,442 i.i.d. samples, with \(N=3,554\) used for training and \(N_*=888\) for testing.
\end{block}

\begin{figure}
	\centering
	\def\TrainEnd{1000}   % split index (x where test starts)
	\def\XMax{1500}       % max x (length of the series)
	
	\begin{tikzpicture}
		\begin{axis}[
			font=\scriptsize,
			width=\textwidth, height=4cm,
			xmin=0, xmax=\XMax,
			axis y line=none,
			axis x line=none,
			xtick=\empty, ytick=\empty,
			grid=none,
			legend cell align=left,
			legend pos=north west,
			clip=false % allow arrow/text outside the plot area
			]
			
			% ---- shaded areas ----
			\path[fill=blue!15]  (rel axis cs:0,0) rectangle (rel axis cs:{\TrainEnd/\XMax},1);
			\path[fill=green!15] (rel axis cs:{\TrainEnd/\XMax},0) rectangle (rel axis cs:1,1);
			
			% ---- example time series (no legend entry) ----
			\addplot[ultra thick, color=black, domain=0:\XMax, samples=600, forget plot]
			{30*sin(deg(x/20)) + 8*sin(deg(x/7)) + 0.03*(x-0.6*\XMax)};
			
			% ---- legend: only shaded areas ----
			\addlegendimage{area legend, fill=blue!15, draw=blue!30}
			\addlegendentry{Training Period}
			\addlegendimage{area legend, fill=green!15, draw=green!30}
			\addlegendentry{Testing Period}
			
			% ---- time direction (arrow + label) ----
			\draw[->, thick] (rel axis cs:0,-0.10) -- (rel axis cs:1,-0.10)
			node[pos=0.5, below] {Time};
			
		\end{axis}
	\end{tikzpicture}
\end{figure}


\begin{block}{}
	The testing dataset \(\mathcal{D}_* = \{\boldsymbol{x}_{*n}, \boldsymbol{y}_{*n}\}_{n=1}^{N_*} \), where \( \boldsymbol{y}_{*n} = [y_{*n,1}, \cdots, y_{*n, D}]^\top \), collects all tasks observations at every test input \( y_{*n,d}\).
\end{block}
	
\end{frame}

\begin{frame}{Model Setup}

\begin{block}{}
	The forecasting methodology constructs the covariance function using the squared exponential kernel, facilitating smooth data mapping.
\end{block}


\[
	k_{q}\left(\boldsymbol{x}, \boldsymbol{x}' \right) = \exp\left(-\frac{1}{2}(\boldsymbol{x} - \boldsymbol{x}')^\top \boldsymbol{\Phi}_q^{-2} (\boldsymbol{x} - \boldsymbol{x}')\right)
\]

\begin{block}{}
\( \boldsymbol{\Phi}_q=\text{diag}\{\Delta_{ql}\}_{l=1}^{L} \in \mathbb{R}^{++L \times L} \) contains the length-scales. Regarding the likelihood,
\end{block}

\[
	p(\boldsymbol{Y} \mid \boldsymbol{\theta}(\boldsymbol{X})) = \prod_{n=1}^{N}\prod_{d=1}^{D} \mathcal{N}\left(y_{nd} \mid h_{d,1}(f_{d,1}(\mathbf{x}_n)), h_{d,2}(f_{d,2}(\mathbf{x}_n)) \right),
\]

\begin{block}{}
$h_{d,1}(\cdot)$ is the identity function. The baseline model is the standard LMC GP with $h_{d,2}(\cdot)$ as a constant, while the proposed model is the Chained LMC GP with $h_{d,2}(\cdot) = \ln(\exp(\cdot)+1)$.
\end{block}

\end{frame}


\begin{frame}{Performance Metrics}

\begin{block}{}
	Given the predictive mean \( \hat{\mu}_{n,d}\) and variance \( \hat{\sigma}^2_{n,d}\), the Mean Squared Error (MSE) takes the following form
\end{block}

\[
	\text{MSE} = \frac{1}{N_{*}D} \sum_{n=1}^{N_*} \sum_{d=1}^{D} \left( y_{*n,d} - \hat{\mu}_{n,d} \right)^2.
\]

\begin{block}{}
The Mean Standardized Log Loss (MSLL) \textcolor{BrandTeal}{\textbf{assesses probabilistic prediction quality}}~\cite{RasmussenW06}
\end{block}

\[
	\text{MSLL} = \frac{1}{2N_{*}D} \sum_{n=1}^{N_*} \sum_{d=1}^{D}  \frac{(y_{*n,d} - \hat{\mu}_{n,d} )^2}{\hat{\sigma}^2_{n,d}}
	- \frac{(y_{*n,d} - \mu_d)^2}{\sigma_d^2} + \ln \left(\frac{\hat{\sigma}^2_{n,d}}{\sigma_d^2}\right)
\]

\begin{block}{}
	where \( \mu_d = \frac{1}{N_*} \sum_{n=1}^{N_*} y_{*n,d}\) and \( \sigma_d^2 = \frac{1}{N_* - 1} \sum_{n=1}^{N_*} (\mu_d - y_{*n,d})^2 \).
\end{block}

\end{frame}

\begin{frame}{Hyperparameter Tuning}
	
	\begin{block}{}	
		A grid search on the baseline model fixed \(T=1\) and \(M=64\). We varied \( Q \in \{1, \cdots ,46\} \).
	\end{block}
	
	\begin{figure}[htbp]
		\centering
		\begin{tikzpicture}[font=\scriptsize]
			\begin{groupplot}[
				group style={
					group size=2 by 1,
					horizontal sep=1.5cm,
					vertical sep=1.5cm,
					xlabels at=edge bottom,
					ylabels at=edge left
				},
				width=0.5\linewidth,
				height=0.32\linewidth,
				xlabel={Number of independent GPs ($Q$)},
				ylabel={MSE},
				grid=both,
				major grid style={line width=.5pt, draw=gray!30},
				minor grid style={line width=.3pt, draw=gray!15},
				tick style={black, thick},
				axis x line=bottom,
				axis y line=left,
				xtick pos=bottom,
				ytick pos=left,
				tick align=outside,
				legend style={
					at={(rel axis cs:0.98,0.98)},
					anchor=north east,
					legend columns=2,
					/tikz/every even column/.append style={column sep=1em},
					inner sep=2pt,
					font=\scriptsize,
					nodes={scale=0.9, transform shape},
					draw=black!25,
					fill=white,
					fill opacity=0.85,
					legend cell align=left
				},
				clip=false
				]
				\nextgroupplot[
				% %xticklabels={},
				% after end axis/.code={
					% 	\node at (rel axis cs:0.5,-0.18) {(a) MSE};
					% }
				]
				\input{figures/MSE_chd_normal.tex}
				\nextgroupplot[
				%xticklabels={},
				ylabel={MSLL},
				xlabel={Number of independent GPs ($Q$})
				% after end axis/.code={
					% 	\node at (rel axis cs:0.5,-0.18) {(b) };
					% }
				]
				\input{figures/MSLL_chd_normal.tex}
				
			\end{groupplot}
		\end{tikzpicture}
	\end{figure}
	
	\begin{block}{}
		The MSE and MSLL decrease until \(Q=15\). Beyond this threshold, the trend becomes unstable, likely due to the increased model complexity.
	\end{block}
	
\end{frame}